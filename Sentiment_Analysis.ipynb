{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT-BR Financial News Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather textual data\n",
    "    - 1 - Test Valor Economico texts \n",
    "    - 2 - Test BDM texts \n",
    "2. Define keywords and phrases\n",
    "    - Automation: How can I automate the process of selecting what is considered relevant?\n",
    "3. Text preprocessing (cleaning and preparing articles)\n",
    "    - Normalize textual data\n",
    "4. Filter articles\n",
    "    - Perform on each article: evaluate for RELEVANT SENTENCES ONLY\n",
    "    - Provide \"irrelevant\" output for futile articles if no sentences hold relevant information\n",
    "5. Sentiment analysis \n",
    "    - Attempt a multi-class classification approach \n",
    "    - 5 categories:\n",
    "        - Good for USD\n",
    "        - Good for BRL\n",
    "        - Neutral\n",
    "6. Trade signals\n",
    "    - Buy USD/BRL\n",
    "    - Sell USD/BRL\n",
    "    - Hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\pt-br\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "finbert_pt_br_tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbert_pt_br_model = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "\n",
    "finbert_pt_br_pipeline = pipeline(task='text-classification', model=finbert_pt_br_model, tokenizer=finbert_pt_br_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          file sentiment     score\n",
      "0    File1.xml  NEGATIVE  0.791573\n",
      "1   File10.xml  NEGATIVE  0.823842\n",
      "2   File11.xml  POSITIVE  0.827733\n",
      "3   File12.xml  NEGATIVE  0.780306\n",
      "4   File13.xml   NEUTRAL  0.671244\n",
      "5   File14.xml  POSITIVE  0.555569\n",
      "6   File15.xml   NEUTRAL  0.551850\n",
      "7   File16.xml  NEGATIVE  0.685905\n",
      "8   File17.xml  POSITIVE  0.439669\n",
      "9   File18.xml  NEGATIVE  0.794118\n",
      "10  File19.xml  NEGATIVE  0.528298\n",
      "11   File2.xml   NEUTRAL  0.536406\n",
      "12   File3.xml  POSITIVE  0.371683\n",
      "13   File4.xml  NEGATIVE  0.750617\n",
      "14   File5.xml  POSITIVE  0.593563\n",
      "15   File6.xml  POSITIVE  0.451566\n",
      "16   File7.xml  NEGATIVE  0.720258\n",
      "17   File8.xml  NEGATIVE  0.831076\n",
      "18   File9.xml  NEGATIVE  0.578087\n"
     ]
    }
   ],
   "source": [
    "def article_classification(directory, max_length=512):\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as fhand:\n",
    "            article = fhand.read()\n",
    "            tokens = finbert_pt_br_pipeline.tokenizer.encode(article, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            if tokens.shape[1] > max_length:\n",
    "                tokens = tokens[:, :max_length]\n",
    "\n",
    "            truncated_text = finbert_pt_br_pipeline.tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            sentiment = finbert_pt_br_pipeline(truncated_text)\n",
    "\n",
    "            classification = {\n",
    "                'file': os.path.basename(filename),\n",
    "                'sentiment': sentiment[0]['label'],\n",
    "                'score': sentiment[0]['score']\n",
    "            }\n",
    "            results.append(classification)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(article_classification('News_Sample/andre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(inputFile, outputFile):\n",
    "    with open(inputFile, 'r', encoding='utf-8') as file:\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            cleaned_line = line.replace('[', '').replace(']', '').replace('â€¦', '').strip()\n",
    "            \n",
    "            if cleaned_line and cleaned_line[2] == '/' and cleaned_line[5] == '/':\n",
    "                if cleaned_lines: \n",
    "                    cleaned_lines.append('')  \n",
    "\n",
    "            if cleaned_line:\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "\n",
    "\n",
    "    with open(outputFile, 'w', encoding='utf-8') as file:\n",
    "        for line in cleaned_lines:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "clean_file('BDM_News_Corpus.txt', 'Clean_BDM_News_Corpus.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
