{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT-BR Financial News Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather textual data\n",
    "    - 1 - Test Valor Economico texts \n",
    "    - 2 - Test BDM texts \n",
    "2. Define keywords and phrases\n",
    "    - Automation: How can I automate the process of selecting what is considered relevant?\n",
    "3. Text preprocessing (cleaning and preparing articles)\n",
    "    - Normalize textual data\n",
    "4. Filter articles\n",
    "    - Perform on each article: evaluate for RELEVANT SENTENCES ONLY\n",
    "    - Provide \"irrelevant\" output for futile articles if no sentences hold relevant information\n",
    "5. Sentiment analysis \n",
    "    - Attempt a multi-class classification approach \n",
    "    - 5 categories:\n",
    "        - Good for USD\n",
    "        - Good for BRL\n",
    "        - Neutral\n",
    "6. Trade signals\n",
    "    - Buy USD/BRL\n",
    "    - Sell USD/BRL\n",
    "    - Hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\pt-br\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "finbert_pt_br_tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbert_pt_br_model = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "\n",
    "finbert_pt_br_pipeline = pipeline(task='text-classification', model=finbert_pt_br_model, tokenizer=finbert_pt_br_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          file sentiment     score\n",
      "0    File1.xml  NEGATIVE  0.791573\n",
      "1   File10.xml  NEGATIVE  0.823842\n",
      "2   File11.xml  POSITIVE  0.827733\n",
      "3   File12.xml  NEGATIVE  0.780306\n",
      "4   File13.xml   NEUTRAL  0.671244\n",
      "5   File14.xml  POSITIVE  0.555569\n",
      "6   File15.xml   NEUTRAL  0.551850\n",
      "7   File16.xml  NEGATIVE  0.685905\n",
      "8   File17.xml  POSITIVE  0.439669\n",
      "9   File18.xml  NEGATIVE  0.794118\n",
      "10  File19.xml  NEGATIVE  0.528298\n",
      "11   File2.xml   NEUTRAL  0.536406\n",
      "12   File3.xml  POSITIVE  0.371683\n",
      "13   File4.xml  NEGATIVE  0.750617\n",
      "14   File5.xml  POSITIVE  0.593563\n",
      "15   File6.xml  POSITIVE  0.451566\n",
      "16   File7.xml  NEGATIVE  0.720258\n",
      "17   File8.xml  NEGATIVE  0.831076\n",
      "18   File9.xml  NEGATIVE  0.578087\n"
     ]
    }
   ],
   "source": [
    "def article_classification(directory, max_length=512):\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as fhand:\n",
    "            article = fhand.read()\n",
    "            tokens = finbert_pt_br_pipeline.tokenizer.encode(article, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            if tokens.shape[1] > max_length:\n",
    "                tokens = tokens[:, :max_length]\n",
    "\n",
    "            truncated_text = finbert_pt_br_pipeline.tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            sentiment = finbert_pt_br_pipeline(truncated_text)\n",
    "\n",
    "            classification = {\n",
    "                'file': os.path.basename(filename),\n",
    "                'sentiment': sentiment[0]['label'],\n",
    "                'score': sentiment[0]['score']\n",
    "            }\n",
    "            results.append(classification)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(article_classification('News_Sample/andre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(inputFile, outputFile):\n",
    "    with open(inputFile, 'r', encoding='utf-8') as file:\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            cleaned_line = line.replace('[', '').replace(']', '').replace('…', '').strip()\n",
    "            \n",
    "            if cleaned_line and cleaned_line[2] == '/' and cleaned_line[5] == '/':\n",
    "                if cleaned_lines: \n",
    "                    cleaned_lines.append('')  \n",
    "\n",
    "            if cleaned_line:\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "\n",
    "\n",
    "    with open(outputFile, 'w', encoding='utf-8') as file:\n",
    "        for line in cleaned_lines:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "clean_file('BDM_News_Corpus.txt', 'Clean_BDM_News_Corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31/01/24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date\n",
       "0   09/01/24\n",
       "1   10/01/24\n",
       "2   11/01/24\n",
       "3   12/01/24\n",
       "4   15/01/24\n",
       "5   16/01/24\n",
       "6   19/01/24\n",
       "7   22/01/24\n",
       "8   23/01/24\n",
       "9   24/01/24\n",
       "10  25/01/24\n",
       "11  26/01/24\n",
       "12  29/01/24\n",
       "13  30/01/24\n",
       "14  31/01/24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plan: Add column for vector label (+1 good for Real, -1 bad for real, 0 meh)\n",
    "1) when we need the dates we will have them\n",
    "2) we will also have info in this prospective second column on whether the currency went up or down for each day in the file\n",
    "3) we can automate this possibly with yahoo finance\n",
    "we refers to me\n",
    "\n",
    "issue: technically the labels in some code further below are in a dictionary so i'll have to figure that out\n",
    "'''\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_dates_from_file(file_path):\n",
    "    date_pattern = r'\\d{2}/\\d{2}/\\d{2}'\n",
    "    dates = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        matches = re.findall(date_pattern, content)\n",
    "        dates.extend(matches)\n",
    "    \n",
    "    dates_df = pd.DataFrame(dates, columns=['Date'])\n",
    "    return dates_df\n",
    "\n",
    "file_path = 'Clean_BDM_News_Corpus.txt'\n",
    "dates_df = extract_dates_from_file(file_path)\n",
    "display(dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>O petróleo testava reação moderada (+0,50%) no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>Circularam comentários de que a reunião de Pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>De qualquer modo, seis senadores estão com a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>Nos EUA, sai a balança comercial de novembro (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>O investidor cumpre a espera pela 5ªF, que pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>Emissão é de apenas uma série e já tem valor d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>ROMI teve lucro líquido de R$ 51,340 milhões n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>ENEVA. Citi manteve recomendação de compra par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>OI. Nova versão do plano de recuperação judici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>LIGHT elegeu Rodrigo Tostes Solon de Pontes co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                            article\n",
       "0     09/01/24  O petróleo testava reação moderada (+0,50%) no...\n",
       "1     09/01/24  Circularam comentários de que a reunião de Pac...\n",
       "2     09/01/24  De qualquer modo, seis senadores estão com a p...\n",
       "3     09/01/24  Nos EUA, sai a balança comercial de novembro (...\n",
       "4     09/01/24  O investidor cumpre a espera pela 5ªF, que pro...\n",
       "...        ...                                                ...\n",
       "1097  31/01/24  Emissão é de apenas uma série e já tem valor d...\n",
       "1098  31/01/24  ROMI teve lucro líquido de R$ 51,340 milhões n...\n",
       "1099  31/01/24  ENEVA. Citi manteve recomendação de compra par...\n",
       "1100  31/01/24  OI. Nova versão do plano de recuperação judici...\n",
       "1101  31/01/24  LIGHT elegeu Rodrigo Tostes Solon de Pontes co...\n",
       "\n",
       "[1102 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# this is where we look at the file and make it into a list\n",
    "def parse_articles_to_df(file_path):\n",
    "    dates = []  # this is where we keep the dates\n",
    "    articles = []  # this is where we keep the articles\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        current_date = None  # this is where we keep the date we are looking at right now\n",
    "        current_articles = []  # this is where we keep the article for the current date\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()  # take away spaces from the start and end\n",
    "            if line:  # if the line is not empty\n",
    "                if line[2] == \"/\":  # if the line looks like a date (dd/mm/yy)\n",
    "                    # we had a date before, so let's save it with its articles\n",
    "                    if current_date:\n",
    "                        for article in current_articles:\n",
    "                            dates.append(current_date)  # add the current date\n",
    "                            articles.append(article)  # add the article for that date\n",
    "                    # just use the date as is, don't change it\n",
    "                    current_date = line  # keep the new date\n",
    "                    current_articles = []  # start fresh for the new date\n",
    "                else:\n",
    "                    # this is the article, we keep adding it to the list\n",
    "                    current_articles.append(line) \n",
    "        \n",
    "        if current_date:  # when we are done looking at the file\n",
    "            for article in current_articles:  # for all the articles we saved\n",
    "                dates.append(current_date)  # add the date again\n",
    "                articles.append(article)  # add the article again\n",
    "    \n",
    "    df = pd.DataFrame({'date': dates, 'article': articles})  # make a table with the dates and articles\n",
    "    return df  # give back the table\n",
    "\n",
    "file_path = \"Clean_BDM_News_Corpus.txt\"  # where the file is\n",
    "df_articles = parse_articles_to_df(file_path)  # call the function to make the table\n",
    "\n",
    "# show the table to see it\n",
    "display(df_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>petróleo testar reação moderar pregão asiático...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>circularam comentário reunião Pacheco líder se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>modo senador presença confirmar Único indicado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>EUA sair balança comercial novembro Fed boy Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>investidor cumprir espera prometer emoção CPI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                  processed_article\n",
       "0  09/01/24  petróleo testar reação moderar pregão asiático...\n",
       "1  09/01/24  circularam comentário reunião Pacheco líder se...\n",
       "2  09/01/24  modo senador presença confirmar Único indicado...\n",
       "3  09/01/24  EUA sair balança comercial novembro Fed boy Mi...\n",
       "4  09/01/24  investidor cumprir espera prometer emoção CPI ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy PT model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "#preprocessing\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # lemmatization and stopwords removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    #tokens back to 1 string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocess ALL articles in df (the file)\n",
    "df_articles['processed_article'] = df_articles['article'].apply(preprocess_text_spacy)\n",
    "\n",
    "# Display the processed articles\n",
    "display(df_articles[['date', 'processed_article']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_articles = df_articles['processed_article'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_articles, \n",
    "                 vector_size=100,   # dimensionality of the word embeddings\n",
    "                 window=5,          # context window size\n",
    "                 min_count=5,       # minimum frequency of words to consider\n",
    "                 workers=4,         # CPUs for training\n",
    "                 sg=0)              # Use CBOW (0) or Skip-Gram (1)\n",
    "\n",
    "model.save(\"word2vec_brl_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>[-0.010041372, 0.027553359, 0.0056391084, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>[-0.0124345375, 0.0380081, 0.004453513, 0.0023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>[-0.008635919, 0.04322502, 0.009430615, -0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>[-0.015746552, 0.05247231, 0.0067171515, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>[-0.02147046, 0.066287816, 0.011573673, 0.0063...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                     article_vector\n",
       "0  09/01/24  [-0.010041372, 0.027553359, 0.0056391084, 0.00...\n",
       "1  09/01/24  [-0.0124345375, 0.0380081, 0.004453513, 0.0023...\n",
       "2  09/01/24  [-0.008635919, 0.04322502, 0.009430615, -0.001...\n",
       "3  09/01/24  [-0.015746552, 0.05247231, 0.0067171515, 0.005...\n",
       "4  09/01/24  [-0.02147046, 0.066287816, 0.011573673, 0.0063..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generating article vectors based on average\n",
    "def get_article_vector(article, model):\n",
    "    tokens = article.split()  # Tokenize the article\n",
    "    word_vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in model.wv:  # if word inn Word2Vec model\n",
    "            word_vectors.append(model.wv[token])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# create article vectors and add them to the df\n",
    "df_articles['article_vector'] = df_articles['processed_article'].apply(lambda x: get_article_vector(x, model))\n",
    "\n",
    "article_vectors = df_articles[['date', 'article_vector']]\n",
    "\n",
    "#if you wanna CSV for better readability\n",
    "article_vectors.to_csv('article_vectors.csv', index=False)\n",
    "\n",
    "display(article_vectors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0 for meh\n",
    "+1 for up\n",
    "-1 for down\n",
    "'''\n",
    "     \n",
    "labels = {\n",
    "    '09/01/24': 1, # 0.8% change\n",
    "    '10/01/24': 0, # 0.2% change\n",
    "    '11/01/24': -1, # 0.6% change\n",
    "    '12/01/24': -1, \n",
    "    '15/01/24': 1,\n",
    "    '16/01/24': 1,\n",
    "    '19/01/24': 0,\n",
    "    '22/01/24': 1,\n",
    "    '23/01/24': -1,\n",
    "    '24/01/24': -1,\n",
    "    '25/01/24': -1,\n",
    "    '26/01/24': -1,\n",
    "    '29/01/24': 1,\n",
    "    '30/01/24': 1,\n",
    "    '31/01/24': 1\n",
    "}\n",
    "\n",
    "# add lablels to df\n",
    "df_articles['label'] = df_articles['date'].map(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.to_csv('article_vectors_with_labels.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\pt-br\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3393665158371041\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score     support\n",
      "-1             0.666667  0.043011  0.080808   93.000000\n",
      "0              0.142857  0.464286  0.218487   28.000000\n",
      "1              0.467742  0.580000  0.517857  100.000000\n",
      "accuracy       0.339367  0.339367  0.339367    0.339367\n",
      "macro avg      0.425755  0.362432  0.272384  221.000000\n",
      "weighted avg   0.510290  0.339367  0.296011  221.000000\n",
      "\n",
      "Confusion Matrix:\n",
      "          Down  Neutral  Up\n",
      "Down        4       38  51\n",
      "Neutral     0       13  15\n",
      "Up          2       40  58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_articles already has processed_article and labels\n",
    "\n",
    "# Step 1: Prepare the data (X = article vectors, y = labels)\n",
    "X = np.vstack(df_articles['article_vector'].values)  # Stack article vectors into a 2D array\n",
    "y = df_articles['label'].values  # Labels corresponding to the article\n",
    "\n",
    "# Step 2: Train-test split (80% for training, 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train a multinomial Logistic Regression classifier\n",
    "clf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Classification report (precision, recall, f1-score)\n",
    "classification_rep = classification_report(y_test, y_pred, output_dict=True)  # Get classification report as a dictionary\n",
    "\n",
    "# Confusion matrix (Actual vs Predicted classes)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert the evaluation results into clean dataframes for display\n",
    "classification_rep_df = pd.DataFrame(classification_rep).transpose()  # Convert classification report to dataframe\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                               index=[\"Down\", \"Neutral\", \"Up\"], \n",
    "                               columns=[\"Down\", \"Neutral\", \"Up\"])  # Confusion matrix as dataframe\n",
    "\n",
    "# Step 6: Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep_df)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_df)\n",
    "\n",
    "# Step 7: Predicting labels for all articles\n",
    "y_pred_all = clf.predict(X)\n",
    "\n",
    "# Assign predictions to the DataFrame\n",
    "df_articles['predicted_label'] = y_pred_all\n",
    "\n",
    "# Display the articles with their predicted labels\n",
    "df_articles.to_csv('article_vectors_with_labels.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
