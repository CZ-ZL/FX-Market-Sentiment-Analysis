{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT-BR Financial News Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages and configuration\n",
    "\n",
    "!pip install transformers torch numpy xmltodict\n",
    "!pip install ipywidgets\n",
    "!pip install --upgrade notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scaro\\Downloads\\pt-br\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "finbert_pt_br_tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbert_pt_br_model = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "\n",
    "finbert_pt_br_pipeline = pipeline(task='text-classification', model=finbert_pt_br_model, tokenizer=finbert_pt_br_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        file sentiment     score\n",
      "0   13794279-7660-4991-8f26-95333f8ca11f.xml  NEGATIVE  0.823842\n",
      "1   1863b43e-cf14-4f9b-9226-7ac40bff2c46.xml  POSITIVE  0.593563\n",
      "2   18af844d-3cab-45b2-97e8-0c30d3a95cc6.xml  NEGATIVE  0.750617\n",
      "3   1a5bca42-e31a-4336-81bd-916b7a3c503c.xml  NEGATIVE  0.791573\n",
      "4   2e21b65c-2d3c-46a4-bd26-93238ba0bbb1.xml   NEUTRAL  0.536406\n",
      "5   31785a5f-5762-4d70-8447-7a5f8799cc8a.xml  NEGATIVE  0.720258\n",
      "6   5948f06f-5fd8-476e-8d7d-b7bf1af16fa0.xml  POSITIVE  0.451566\n",
      "7   66061c43-3cf9-4a92-a72c-b55c2bf23324.xml  NEGATIVE  0.831076\n",
      "8   85931aef-92f7-4e24-b9bf-4a0949f041d4.xml  NEGATIVE  0.578087\n",
      "9   9ad2c6c3-7c2c-45d4-8b19-da699c9eb335.xml  POSITIVE  0.371683\n",
      "10  a4930391-601d-458b-af1a-3e0af3a7cc59.xml  POSITIVE  0.827733\n",
      "11  b51b53b8-34cb-4a83-b928-e2b8f5277377.xml  NEGATIVE  0.780306\n",
      "12  bfbd2175-164e-4f02-b136-0685c1c53dc1.xml   NEUTRAL  0.671244\n",
      "13  c1667639-f956-4794-b03f-03f6042f31e6.xml  NEGATIVE  0.685905\n",
      "14  c1d0c39f-de76-4442-ad3f-3d68652d0778.xml  POSITIVE  0.555569\n",
      "15  c731a323-0e13-41d8-babd-8da7a6fce001.xml   NEUTRAL  0.551850\n",
      "16  cd35d29f-a5e5-4bbe-bda7-7dc6a35462fc.xml  POSITIVE  0.439669\n",
      "17  dde8f8c1-6485-432f-9837-e7098c19610e.xml  NEGATIVE  0.794118\n",
      "18  e87eb152-02b9-4670-b0fb-3707fd2e3206.xml  NEGATIVE  0.528298\n"
     ]
    }
   ],
   "source": [
    "def article_classification(directory, max_length=512):\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as fhand:\n",
    "            article = fhand.read()\n",
    "            tokens = tokenizer.encode(article, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            if tokens.shape[1] > max_length:\n",
    "                tokens = tokens[:, :max_length]\n",
    "\n",
    "            truncated_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            sentiment = finbert_pt_br_pipeline(truncated_text)\n",
    "\n",
    "            classification = {\n",
    "                'file': os.path.basename(filename),\n",
    "                'sentiment': sentiment[0]['label'],\n",
    "                'score': sentiment[0]['score']\n",
    "            }\n",
    "            results.append(classification)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(article_classification('News_Sample/andre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
