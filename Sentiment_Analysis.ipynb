{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT-BR Financial News Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather textual data\n",
    "    - 1 - Test Valor Economico texts \n",
    "    - 2 - Test BDM texts \n",
    "2. Define keywords and phrases\n",
    "    - Automation: How can I automate the process of selecting what is considered relevant?\n",
    "3. Text preprocessing (cleaning and preparing articles)\n",
    "    - Normalize textual data\n",
    "4. Filter articles\n",
    "    - Perform on each article: evaluate for RELEVANT SENTENCES ONLY\n",
    "    - Provide \"irrelevant\" output for futile articles if no sentences hold relevant information\n",
    "5. Sentiment analysis \n",
    "    - Attempt a multi-class classification approach \n",
    "    - 5 categories:\n",
    "        - Good for USD\n",
    "        - Good for BRL\n",
    "        - Neutral\n",
    "6. Trade signals\n",
    "    - Buy USD/BRL\n",
    "    - Sell USD/BRL\n",
    "    - Hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "finbert_pt_br_tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbert_pt_br_model = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "\n",
    "finbert_pt_br_pipeline = pipeline(task='text-classification', model=finbert_pt_br_model, tokenizer=finbert_pt_br_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          file sentiment     score\n",
      "0    File1.xml  NEGATIVE  0.791573\n",
      "1   File10.xml  NEGATIVE  0.823842\n",
      "2   File11.xml  POSITIVE  0.827733\n",
      "3   File12.xml  NEGATIVE  0.780306\n",
      "4   File13.xml   NEUTRAL  0.671244\n",
      "5   File14.xml  POSITIVE  0.555569\n",
      "6   File15.xml   NEUTRAL  0.551850\n",
      "7   File16.xml  NEGATIVE  0.685905\n",
      "8   File17.xml  POSITIVE  0.439669\n",
      "9   File18.xml  NEGATIVE  0.794118\n",
      "10  File19.xml  NEGATIVE  0.528298\n",
      "11   File2.xml   NEUTRAL  0.536406\n",
      "12   File3.xml  POSITIVE  0.371683\n",
      "13   File4.xml  NEGATIVE  0.750617\n",
      "14   File5.xml  POSITIVE  0.593563\n",
      "15   File6.xml  POSITIVE  0.451566\n",
      "16   File7.xml  NEGATIVE  0.720258\n",
      "17   File8.xml  NEGATIVE  0.831076\n",
      "18   File9.xml  NEGATIVE  0.578087\n"
     ]
    }
   ],
   "source": [
    "def article_classification(directory, max_length=512):\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as fhand:\n",
    "            article = fhand.read()\n",
    "            tokens = finbert_pt_br_pipeline.tokenizer.encode(article, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            if tokens.shape[1] > max_length:\n",
    "                tokens = tokens[:, :max_length]\n",
    "\n",
    "            truncated_text = finbert_pt_br_pipeline.tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            sentiment = finbert_pt_br_pipeline(truncated_text)\n",
    "\n",
    "            classification = {\n",
    "                'file': os.path.basename(filename),\n",
    "                'sentiment': sentiment[0]['label'],\n",
    "                'score': sentiment[0]['score']\n",
    "            }\n",
    "            results.append(classification)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(article_classification('News_Sample/andre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(inputFile, outputFile):\n",
    "    with open(inputFile, 'r', encoding='utf-8') as file:\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            cleaned_line = line.replace('[', '').replace(']', '').replace('…', '').strip()\n",
    "            \n",
    "            if cleaned_line and cleaned_line[2] == '/' and cleaned_line[5] == '/':\n",
    "                if cleaned_lines: \n",
    "                    cleaned_lines.append('')  \n",
    "\n",
    "            if cleaned_line:\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "\n",
    "\n",
    "    with open(outputFile, 'w', encoding='utf-8') as file:\n",
    "        for line in cleaned_lines:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "clean_file('BDM_News_Corpus.txt', 'Clean_BDM_News_Corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>O petróleo testava reação moderada (+0,50%) no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>Circularam comentários de que a reunião de Pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>De qualquer modo, seis senadores estão com a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>Nos EUA, sai a balança comercial de novembro (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>O investidor cumpre a espera pela 5ªF, que pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                            article\n",
       "0  01/09/24  O petróleo testava reação moderada (+0,50%) no...\n",
       "1  01/09/24  Circularam comentários de que a reunião de Pac...\n",
       "2  01/09/24  De qualquer modo, seis senadores estão com a p...\n",
       "3  01/09/24  Nos EUA, sai a balança comercial de novembro (...\n",
       "4  01/09/24  O investidor cumpre a espera pela 5ªF, que pro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#preprocessing and cleaning\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to parse the text file and return a Pandas DataFrame with date format conversion\n",
    "def parse_articles_to_df(file_path):\n",
    "    dates = []\n",
    "    articles = []\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        current_date = None\n",
    "        current_articles = []\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Ignore empty lines\n",
    "                if line[2] == \"/\":  # Date format starts with dd/mm/yy\n",
    "                    # If we find a date, save the previous articles\n",
    "                    if current_date:\n",
    "                        for article in current_articles:\n",
    "                            dates.append(current_date)\n",
    "                            articles.append(article)\n",
    "                    # Convert date to MM/DD/YY format (from DD/MM/YY)\n",
    "                    current_date = convert_brazilian_to_american_date(line)\n",
    "                    current_articles = []\n",
    "                else:\n",
    "                    # Add the article text under the current date\n",
    "                    current_articles.append(line)\n",
    "        \n",
    "        # Save the last group of articles\n",
    "        if current_date:\n",
    "            for article in current_articles:\n",
    "                dates.append(current_date)\n",
    "                articles.append(article)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'date': dates, 'article': articles})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to convert Brazilian date format (DD/MM/YY) to American format (MM/DD/YY)\n",
    "def convert_brazilian_to_american_date(date_str):\n",
    "    # Parse the Brazilian date (DD/MM/YY)\n",
    "    date_obj = datetime.strptime(date_str, \"%d/%m/%y\")\n",
    "    \n",
    "    # Convert to American date format (MM/DD/YY)\n",
    "    return date_obj.strftime(\"%m/%d/%y\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Clean_BDM_News_Corpus.txt\"\n",
    "df_articles = parse_articles_to_df(file_path)\n",
    "\n",
    "# Display first few rows of the DataFrame to check\n",
    "display(df_articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>petróleo testar reação moderar pregão asiático...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>circularam comentário reunião Pacheco líder se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>modo senador presença confirmar Único indicado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>EUA sair balança comercial novembro Fed boy Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/09/24</td>\n",
       "      <td>investidor cumprir espera prometer emoção CPI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                  processed_article\n",
       "0  01/09/24  petróleo testar reação moderar pregão asiático...\n",
       "1  01/09/24  circularam comentário reunião Pacheco líder se...\n",
       "2  01/09/24  modo senador presença confirmar Único indicado...\n",
       "3  01/09/24  EUA sair balança comercial novembro Fed boy Mi...\n",
       "4  01/09/24  investidor cumprir espera prometer emoção CPI ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the Portuguese model for spaCy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Preprocessing function using spaCy\n",
    "def preprocess_text_spacy(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize and remove stopwords and non-alphabetic tokens\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all articles in the DataFrame\n",
    "df_articles['processed_article'] = df_articles['article'].apply(preprocess_text_spacy)\n",
    "\n",
    "# Display the processed articles\n",
    "display(df_articles[['date', 'processed_article']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize all the processed articles into a list of tokenized words\n",
    "tokenized_articles = df_articles['processed_article'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_articles, \n",
    "                 vector_size=100,   # Dimensionality of the word embeddings\n",
    "                 window=5,          # Context window size\n",
    "                 min_count=5,       # Minimum frequency of words to consider\n",
    "                 workers=4,         # Number of threads for training\n",
    "                 sg=0)              # Use CBOW (0) or Skip-Gram (1)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec_brl_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to generate an article vector by averaging the word vectors\n",
    "def get_article_vector(article, model):\n",
    "    tokens = article.split()  # Tokenize the article\n",
    "    word_vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in model.wv:  # Check if the word exists in the Word2Vec model\n",
    "            word_vectors.append(model.wv[token])\n",
    "    \n",
    "    # If we have word vectors, return their average; otherwise, return a zero vector\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Generate article vectors and add them to the DataFrame\n",
    "df_articles['article_vector'] = df_articles['processed_article'].apply(lambda x: get_article_vector(x, model))\n",
    "\n",
    "# Display the DataFrame with article vectors\n",
    "article_vectors = df_articles[['date', 'article_vector']]\n",
    "article_vectors.to_csv('article_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"01/09/24\": 1,  # +1 for up\n",
    "    \"01/10/24\": -1, # -1 for down\n",
    "    \"01/15/24\": 0   # 0 for meh\n",
    "}\n",
    "\n",
    "# Add labels to the DataFrame\n",
    "df_articles['label'] = df_articles['date'].map(labels)\n",
    "\n",
    "# Display the DataFrame with labels\n",
    "print(df_articles[['date', 'article', 'label']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
