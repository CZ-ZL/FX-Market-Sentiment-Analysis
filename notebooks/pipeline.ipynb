{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation\n",
    "- applied labels by observing whether the USD/BRL exchange rate increased, decreased, or experienced low volatility (neutral)\n",
    "- applied threshold of 0.2% applied to manually determining neutral days (technically doesn't even matter because we're not using the above approach anymore. Using Eli's Labels instead)\n",
    "- applied word2vec model to vectorize text and further classify new articles to generate buy/sell/hold signals\n",
    "\n",
    "Notes (in-progress)\n",
    "- avoid data-leakage: don't train/test on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Dates in articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30/01/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31/01/24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date\n",
       "0   09/01/24\n",
       "1   10/01/24\n",
       "2   11/01/24\n",
       "3   12/01/24\n",
       "4   15/01/24\n",
       "5   16/01/24\n",
       "6   19/01/24\n",
       "7   22/01/24\n",
       "8   23/01/24\n",
       "9   24/01/24\n",
       "10  25/01/24\n",
       "11  26/01/24\n",
       "12  29/01/24\n",
       "13  30/01/24\n",
       "14  31/01/24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking what dates are in our file filled with articles\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_dates_from_file(file_path):\n",
    "    date_pattern = r'\\d{2}/\\d{2}/\\d{2}'\n",
    "    dates = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        matches = re.findall(date_pattern, content)\n",
    "        dates.extend(matches)\n",
    "    \n",
    "    dates_df = pd.DataFrame(dates, columns=['Date'])\n",
    "    return dates_df\n",
    "\n",
    "file_path = 'data/news_corpus_cleaned.txt'\n",
    "dates_df = extract_dates_from_file(file_path)\n",
    "display(dates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing article file data into a DataFrame for readability and easier to convert to CSV\n",
    "- PLEASE SAVE THIS OUTPUTTED DATA AS A CSV\n",
    "- this is what i manually did for january, not exactly reproducable, be careful with this, i built the dataframe for february differently\n",
    "- i need to get every data source into this dataframe format because it's easy to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINSIHED PREPROCESSING JANUARY DATA, SAVED AS labeled_january_data.csv now, keeping this code for reusability later on\n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from datetime import datetime\n",
    "\n",
    "# # look at the file and make it into a list\n",
    "# def parse_articles_to_df(file_path):\n",
    "#     dates = []  # keep the dates\n",
    "#     articles = []  # keep the articles\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         current_date = None  # keep the date we are looking at right now\n",
    "#         current_articles = []  # keep the article for the current date\n",
    "        \n",
    "#         for line in file:\n",
    "#             line = line.strip()  # take away spaces from the start and end\n",
    "#             if line:  # if the line is not empty\n",
    "#                 if line[2] == \"/\":  # if the line looks like a date (dd/mm/yy)\n",
    "#                     # we had a date before, so let's save it with its articles\n",
    "#                     if current_date:\n",
    "#                         for article in current_articles:\n",
    "#                             dates.append(current_date)  # add current date\n",
    "#                             articles.append(article)  # add article for that date\n",
    "#                     # use the date as is, don't change it\n",
    "#                     current_date = line  # keep new date\n",
    "#                     current_articles = []  # start fresh for new date\n",
    "#                 else:\n",
    "#                     # 'line' is the article, keep adding them to the list\n",
    "#                     current_articles.append(line) \n",
    "        \n",
    "#         if current_date:  # when we are done looking at the file\n",
    "#             for article in current_articles:  # for all the articles we saved\n",
    "#                 dates.append(current_date)  # add the date again\n",
    "#                 articles.append(article)  # add the article again\n",
    "    \n",
    "#     df = pd.DataFrame({'date': dates, 'article': articles})  # make table with the dates and articles\n",
    "#     return df  # display table\n",
    "\n",
    "# file_path = \"data/news_corpus_cleaned.txt\"  # where the file is\n",
    "# df_template = parse_articles_to_df(file_path)  # call function to make the table, called df_template because it can be used for another month\n",
    "\n",
    "# # show the table to see it\n",
    "# display(df_template.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## January Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/labeled_january_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>O petróleo testava reação moderada (+0,50%) no...</td>\n",
       "      <td>0</td>\n",
       "      <td>petróleo testar reação moderar pregão asiático...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>Circularam comentários de que a reunião de Pac...</td>\n",
       "      <td>0</td>\n",
       "      <td>circularam comentário reunião Pacheco líder se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>De qualquer modo, seis senadores estão com a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>modo senador presença confirmar Único indicado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>Nos EUA, sai a balança comercial de novembro (...</td>\n",
       "      <td>-1</td>\n",
       "      <td>EUA sair balança comercial novembro Fed boy Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/01/24</td>\n",
       "      <td>O investidor cumpre a espera pela 5ªF, que pro...</td>\n",
       "      <td>-1</td>\n",
       "      <td>investidor cumprir espera prometer emoção CPI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>Emissão é de apenas uma série e já tem valor d...</td>\n",
       "      <td>0</td>\n",
       "      <td>Emissão série definir revelar executivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>ROMI teve lucro líquido de R$ 51,340 milhões n...</td>\n",
       "      <td>0</td>\n",
       "      <td>ROMI lucro líquido milhão queda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>ENEVA. Citi manteve recomendação de compra par...</td>\n",
       "      <td>0</td>\n",
       "      <td>ENEVA Citi manter recomendação compra ação ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>OI. Nova versão do plano de recuperação judici...</td>\n",
       "      <td>0</td>\n",
       "      <td>OI versão plano recuperação judicial concluir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>31/01/24</td>\n",
       "      <td>LIGHT elegeu Rodrigo Tostes Solon de Pontes co...</td>\n",
       "      <td>0</td>\n",
       "      <td>LIGHT elegeu Rodrigo Tostes Solon Pontes diret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                            article  label  \\\n",
       "0     09/01/24  O petróleo testava reação moderada (+0,50%) no...      0   \n",
       "1     09/01/24  Circularam comentários de que a reunião de Pac...      0   \n",
       "2     09/01/24  De qualquer modo, seis senadores estão com a p...      0   \n",
       "3     09/01/24  Nos EUA, sai a balança comercial de novembro (...     -1   \n",
       "4     09/01/24  O investidor cumpre a espera pela 5ªF, que pro...     -1   \n",
       "...        ...                                                ...    ...   \n",
       "1096  31/01/24  Emissão é de apenas uma série e já tem valor d...      0   \n",
       "1097  31/01/24  ROMI teve lucro líquido de R$ 51,340 milhões n...      0   \n",
       "1098  31/01/24  ENEVA. Citi manteve recomendação de compra par...      0   \n",
       "1099  31/01/24  OI. Nova versão do plano de recuperação judici...      0   \n",
       "1100  31/01/24  LIGHT elegeu Rodrigo Tostes Solon de Pontes co...      0   \n",
       "\n",
       "                                      processed_article  \n",
       "0     petróleo testar reação moderar pregão asiático...  \n",
       "1     circularam comentário reunião Pacheco líder se...  \n",
       "2     modo senador presença confirmar Único indicado...  \n",
       "3     EUA sair balança comercial novembro Fed boy Mi...  \n",
       "4     investidor cumprir espera prometer emoção CPI ...  \n",
       "...                                                 ...  \n",
       "1096            Emissão série definir revelar executivo  \n",
       "1097                    ROMI lucro líquido milhão queda  \n",
       "1098  ENEVA Citi manter recomendação compra ação ban...  \n",
       "1099  OI versão plano recuperação judicial concluir ...  \n",
       "1100  LIGHT elegeu Rodrigo Tostes Solon Pontes diret...  \n",
       "\n",
       "[1101 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy PT model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "#preprocessing\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # lemmatization and stopwords removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    #tokens back to 1 string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocess ALL articles in df (the file)\n",
    "df_jan['processed_article'] = df_jan['article'].apply(preprocess_text_spacy)\n",
    "\n",
    "# Display the processed articles\n",
    "display(df_jan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## February Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/labeled_february_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>… O PMI industrial chinês medido pelo setor pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>PMI industrial chinês meder setor privado fica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>… O texto do BC, praticamente igual ao anterio...</td>\n",
       "      <td>0</td>\n",
       "      <td>texto BC praticamente igual anterior dezembro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>… Depois de baixar a Selic para 11,25%, o BC n...</td>\n",
       "      <td>1</td>\n",
       "      <td>baixar Selic BC mexeu quase comunicado parágra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>… O Copom não encurtou o horizonte de cortes, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Copom encurtar horizonte corte manter barra al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>… Isso significa que março continua dado e que...</td>\n",
       "      <td>1</td>\n",
       "      <td>significar março continuar dar maio reservar s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>28/02/2024</td>\n",
       "      <td>CPFL PAULISTA. Conselho aprovou 14ª emissão de...</td>\n",
       "      <td>0</td>\n",
       "      <td>CPFL PAULISTA aprovar emissão debêntur montant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>28/02/2024</td>\n",
       "      <td>CPFL PIRATININGA. Conselho aprovou 16ª emissão...</td>\n",
       "      <td>0</td>\n",
       "      <td>CPFL PIRATININGA aprovar emissão debêntur mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>28/02/2024</td>\n",
       "      <td>UNIPAR informou a renúncia de Antonio Rabello,...</td>\n",
       "      <td>0</td>\n",
       "      <td>UNIPAR informar renúncia Antonio Rabello diret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>28/02/2024</td>\n",
       "      <td>GRUPO MATEUS concluiu venda de cinco imóveis p...</td>\n",
       "      <td>0</td>\n",
       "      <td>MATEUS concluir venda imóvel fundo TRX real mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>28/02/2024</td>\n",
       "      <td>SYN PROP &amp; TECH, antiga Cyrela Commercial Prop...</td>\n",
       "      <td>0</td>\n",
       "      <td>SYN PROP TECH antigo Cyrela Commercial Propert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>919 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                            article  label  \\\n",
       "0    01/02/2024  … O PMI industrial chinês medido pelo setor pr...      0   \n",
       "1    01/02/2024  … O texto do BC, praticamente igual ao anterio...      0   \n",
       "2    01/02/2024  … Depois de baixar a Selic para 11,25%, o BC n...      1   \n",
       "3    01/02/2024  … O Copom não encurtou o horizonte de cortes, ...      1   \n",
       "4    01/02/2024  … Isso significa que março continua dado e que...      1   \n",
       "..          ...                                                ...    ...   \n",
       "914  28/02/2024  CPFL PAULISTA. Conselho aprovou 14ª emissão de...      0   \n",
       "915  28/02/2024  CPFL PIRATININGA. Conselho aprovou 16ª emissão...      0   \n",
       "916  28/02/2024  UNIPAR informou a renúncia de Antonio Rabello,...      0   \n",
       "917  28/02/2024  GRUPO MATEUS concluiu venda de cinco imóveis p...      0   \n",
       "918  28/02/2024  SYN PROP & TECH, antiga Cyrela Commercial Prop...      0   \n",
       "\n",
       "                                     processed_article  \n",
       "0    PMI industrial chinês meder setor privado fica...  \n",
       "1    texto BC praticamente igual anterior dezembro ...  \n",
       "2    baixar Selic BC mexeu quase comunicado parágra...  \n",
       "3    Copom encurtar horizonte corte manter barra al...  \n",
       "4    significar março continuar dar maio reservar s...  \n",
       "..                                                 ...  \n",
       "914  CPFL PAULISTA aprovar emissão debêntur montant...  \n",
       "915  CPFL PIRATININGA aprovar emissão debêntur mont...  \n",
       "916  UNIPAR informar renúncia Antonio Rabello diret...  \n",
       "917  MATEUS concluir venda imóvel fundo TRX real mi...  \n",
       "918  SYN PROP TECH antigo Cyrela Commercial Propert...  \n",
       "\n",
       "[919 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy PT model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "#preprocessing\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # lemmatization and stopwords removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    #tokens back to 1 string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocess ALL articles in df (the file)\n",
    "df_feb['processed_article'] = df_feb['article'].apply(preprocess_text_spacy)\n",
    "\n",
    "# Display the processed articles\n",
    "display(df_feb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Word2Vec approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_articles = df_articles['processed_article'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_articles, \n",
    "                 vector_size=100,   # dimensionality of the word embeddings\n",
    "                 window=5,          # context window size\n",
    "                 min_count=5,       # minimum frequency of words to consider\n",
    "                 workers=4,         # CPUs for training\n",
    "                 sg=0)              # Use CBOW (0) or Skip-Gram (1)\n",
    "\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling of all articles on trading day \"d\" based on whether the USD/BRL exchange rate increased/decreased or hovered around a .2% increase/decrease on trading day \"d\"\n",
    "- Increase: +1\n",
    "- Decrease: -1\n",
    "- Neutral: 0\n",
    "\n",
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Word2Vec approach by applying +1, -1, 0 labels to ALL articles per trading day whether the BRL went up down or stayed around the same based on a .2% threshold.\n",
    "# 0 for neutral\n",
    "# +1 for up\n",
    "# -1 for down\n",
    "\n",
    "labels = {\n",
    "    '09/01/24': 1,\n",
    "    '10/01/24': 0,\n",
    "    '11/01/24': -1,\n",
    "    '12/01/24': -1, \n",
    "    '15/01/24': 1,\n",
    "    '16/01/24': 1,\n",
    "    '19/01/24': 0,\n",
    "    '22/01/24': 1,\n",
    "    '23/01/24': -1,\n",
    "    '24/01/24': -1,\n",
    "    '25/01/24': -1,\n",
    "    '26/01/24': -1,\n",
    "    '29/01/24': 1,\n",
    "    '30/01/24': 1,\n",
    "    '31/01/24': 1\n",
    "}\n",
    "\n",
    "# add lablels to df\n",
    "df_template['label'] = df_template['date'].map(labels) # called df_template because I might use this in the future for any month\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import itertools  # needed this for combinations\n",
    "\n",
    "# function to get the article vector by averaging word vectors\n",
    "def get_article_vector(article, model):\n",
    "    tokens = article.split()  # split the article into words\n",
    "    # get the word vectors for each word that exists in the model's vocabulary\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:  # if there are any word vectors\n",
    "        return np.mean(vectors, axis=0)  # average them to get a single vector\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # if no word vectors, return a zero vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "January Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all articles into vectors\n",
    "df_jan['article_vector'] = df_jan['processed_article'].apply(lambda x: get_article_vector(x, model))\n",
    "\n",
    "labels = df_jan['label'].values  # get the labels\n",
    "\n",
    "# Checking the data we're working with before feeding it into the model\n",
    "df_jan.to_csv('vectorized_january_data.csv')\n",
    "\n",
    "display(df_jan.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "February Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feb['article_vector'] = df_feb['processed_article'].apply(lambda x: get_article_vector(x, model))\n",
    "labels = df_feb['label'].values\n",
    "\n",
    "df_feb.to_csv('vectorized_january_data.csv')\n",
    "\n",
    "display(df_feb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression Model with Custom Word2Vec Model\n",
    "\n",
    "Next steps:\n",
    "- Apply softmax to improve accuracy\n",
    "- Train on January, test on first 2 weeks of February\n",
    "    - Or grab equal amounts of each label and make sure to test on NEW articles since the training data may come from spread out dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare the data (X = article vectors, y = labels)\n",
    "X = np.vstack(df_articles['article_vector'].values)\n",
    "y = df_articles['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    C=1.0,\n",
    "    penalty='l2'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "X_all = np.vstack(df_articles['article_vector'].values)\n",
    "predicted_labels = clf.predict(X_all)\n",
    "df_articles['predicted_label'] = predicted_labels\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Classification report\n",
    "classification_rep = classification_report(y_test, y_pred, output_dict=True)  # Get classification report as dictionary\n",
    "\n",
    "# Convert evaluation results into DataFrames for better display\n",
    "classification_rep_df = pd.DataFrame(classification_rep).transpose()\n",
    "\n",
    "# Step 6: Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "display(\"Classification Report:\")\n",
    "display(classification_rep_df)\n",
    "\n",
    "display(df_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_test is actual labels and y_pred is predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
