{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/labeled_january_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/labeled_february_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy PT model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "#preprocessing\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # lemmatization and stopwords removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    #tokens back to 1 string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocess ALL articles in january's dataframe\n",
    "df_jan['preprocessed_article'] = df_jan['article'].apply(preprocess_text_spacy)\n",
    "df_feb['preprocessed_article'] = df_feb['article'].apply(preprocess_text_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Word2Vec approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model trained on combined datasets and saved to models/word2vec_combined.model.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(df_list, vector_size=300, window=5, min_count=5, epochs=20, sg=1):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec model using tokenized articles from multiple dataframes.\n",
    "    :param df_list: List of dataframes containing a 'preprocessed_article' column.\n",
    "    :param vector_size: Dimensionality of word vectors.\n",
    "    :param window: Maximum distance between current and predicted word.\n",
    "    :param min_count: Ignores words with total frequency lower than this.\n",
    "    :param epochs: Number of training iterations.\n",
    "    :param sg: Skip-gram method (1) or CBOW (0).\n",
    "    :return: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    # Combine tokenized articles from all dataframes\n",
    "    tokenized_articles = []\n",
    "    for df in df_list:\n",
    "        tokenized_articles.extend(df['preprocessed_article'])\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=tokenized_articles,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=sg,\n",
    "        workers=4,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    return word2vec_model\n",
    "\n",
    "# Train Word2Vec model on both datasets\n",
    "word2vec_model = train_word2vec([df_jan, df_feb])\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save(\"models/word2vec_combined.model\")\n",
    "print(\"Word2Vec model trained on combined datasets and saved to models/word2vec_combined.model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the distribution of the Target variable\n",
    "- helps us to realize a class imbalance\n",
    "- good to keep track of\n",
    "- MOVE THIS INTO the PREPROCESSING file eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_jan)\n",
    "plt.title('Label Distribution in January Dataset')\n",
    "plt.savefig(\"results/label_distribution_january.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_feb)\n",
    "plt.title('Label Distribution in February Dataset')\n",
    "plt.savefig(\"results/label_distribution_february.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(np.vstack(df_feb['embedding']))\n",
    "\n",
    "# Add 2D coordinates to the DataFrame\n",
    "df_feb['embedding_2d_x'] = embeddings_2d[:, 0]\n",
    "df_feb['embedding_2d_y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE stuff\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='embedding_2d_x', y='embedding_2d_y', hue='label',\n",
    "    data=df_feb, palette='viridis', legend='full'\n",
    ")\n",
    "plt.title('t-SNE Visualization of February Dataset Embeddings')\n",
    "plt.savefig(\"results/tsne_visualization_february.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(np.vstack(df_jan['embedding']))\n",
    "\n",
    "# Add 2D coordinates to the DataFrame\n",
    "df_jan['embedding_2d_x'] = embeddings_2d[:, 0]\n",
    "df_jan['embedding_2d_y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Plot t-SNE stuff\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='embedding_2d_x', y='embedding_2d_y', hue='label',\n",
    "    data=df_jan, palette='viridis', legend='full'\n",
    ")\n",
    "plt.title('t-SNE Visualization of January Dataset Embeddings')\n",
    "plt.savefig(\"results/tsne_visualization_january.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# January\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/processed/labeled_january_data.csv'\n",
    "df_jan = pd.read_csv(file_path)\n",
    "\n",
    "column_data = df_jan['label']\n",
    "counts = column_data.value_counts()\n",
    "\n",
    "count_1 = counts[1]\n",
    "count_minus_1 = counts[-1]\n",
    "count_0 = counts[0]\n",
    "\n",
    "print(\"January\\n\")\n",
    "print(f\"Count of 1: {count_1}\")\n",
    "print(f\"Count of -1: {count_minus_1}\")\n",
    "print(f\"Count of 0: {count_0}\")\n",
    "print(f\"Total Number of Articles: {count_1 + count_minus_1 + count_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# February\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/processed/labeled_february_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "column_name = 'label'\n",
    "column_data = df[column_name]\n",
    "\n",
    "counts = column_data.value_counts()\n",
    "\n",
    "count_1 = counts[1]\n",
    "count_minus_1 = counts[-1]\n",
    "count_0 = counts[0]\n",
    "\n",
    "\n",
    "print(\"February\\n\")\n",
    "print(f\"Count of 1: {count_1}\")\n",
    "print(f\"Count of -1: {count_minus_1}\")\n",
    "print(f\"Count of 0: {count_0}\")\n",
    "print(f\"Total Number of Articles: {count_1 + count_minus_1 + count_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression Model with Custom Word2Vec Model\n",
    "\n",
    "Task: \n",
    "- Train on January, test on first 2 weeks of February respectively\n",
    "    - Train / test with 3 classes (+1, 0, -1), yielding a 3 x 3 confusion matrix\n",
    "    - Train / test with 2 classes (+1, -1), yielding a 2 x 2 confusion matrix\n",
    "- Apply softmax to improve accuracy if poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_test is actual labels and y_pred is predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_test is actual labels and y_pred is predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
